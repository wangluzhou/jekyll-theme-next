---
title: 梯度下降
date: 2017-10-25 23:42:21
categories:
- 技术
tags:
- 深度学习
---
[TOC]

# Naive Bayes和Gradient Descent

## 情绪的理解
情绪的分析最基本的判断就是正面和负面的划分。
[snwnlp语料库](https://github.com/isnowfy/snownlp)
snowNLP是一个Python写的类库，可以方便地处理中文文本内容，一个方便用于处理中文的类库，并且没有NLTK（自然语言处理包），所有的算法都是自己实现的，并且自带了一些训练好了的字典。本程序都是处理unicode编码的，需要自行decode和unicode。

>推荐英文论文:Speech and Language Processing - ch6 - naive bayes and sentiment classification

情感分类可以分为正面和负面，更为细致的我们可以将其分为兴奋、悲伤等等。更一般的我们利用机器学习里面的有监督学习，通过人为标签告诉机器什么句子是正面的，什么句子是负面的。

## 有监督学习
有监督问题可以分为
- 回归问题：标签（因变量）是实数
- 分类问题：标签（因变量）是有限集合

当你不限定搜索集合的时候，最好的拟合模型就是数据本身。有监督学习是**限定集合**里的最优模型。

>[Andrew Ng描述有监督学习](https://www.coursera.org/learn/machine-learning/lecture/1VkCb/supervised-learning)

接下来我们来看看各个算法是如何设定所谓的限定集合的（有限参数可调整的函数）。

- 贝叶斯分类器

$$ P(y=emotion|S)=P(y|w_1, w_2, w_3, ..., w_n) $$
$$ f(S)=arg\ \max_{y} P(y|S)  $$
其中S表示句子。
因为条件不可拆解，所以我们可以利用贝叶斯公式，将情绪做条件，变成
$$ P(y|S)=\frac{P(S|y)P(y)}{P(S)} $$
以情绪为条件的不同句子的概率分布。
其中的分母P(S)并不需要去计算，不同的emotion，P(S)是一样的。
叠加词汇之间的独立性假设，我们就可以得到我们的朴素贝叶斯模型：
$$ P(S|y=emotion)=P(w_1, w_2, w_3, ..., w_n|y=emotion)
\approx P(w_1|y=emtion)...P(w_n|y=emotion) $$

注意：通过独立性假设之后，我们可以直接得到句子的情感。不需要任何的拟合优化，非常简单。

> **疑问**：
*为什么贝叶斯分类器分母P(S)在不同的emotion下都是一样的？*
答：因为这个公式的变量只有y，你要理解这个公式要解决什么给你一个句子，你要判断它属于各个情绪的概率，那么很显然，你要求的是y的概率分布，S是作为一种已知的常量代入的。即使我们不知道P(S)，最后也只需要利用所有y的概率表达式的和等于1的限制条件，就可以反算出P(S)的值。



梳理一下，朴素贝叶斯和一般贝叶斯分类器有什么区别？它为什么too naive?
答：朴素贝叶斯假设一个句子中词汇之间是没有相关性的，方便对贝叶斯公式进行分解，
>[数学之美番外篇：平凡而又神奇的贝叶斯方法 – 刘未鹏 | Mind Hacks](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/)
[朴素贝叶斯分类器的应用 - 阮一峰](http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html)

特征工程：通过人的知识，对数据进行拆解分析，相当于人为模型服务。在真正的数据挖掘项目中，数据标准化，特征工程以及模型设计是花时间最多的工作，一旦这些东西工作完成，剩下的用训练测试可能只占到整个宏观数据挖掘工程的5%左右。

缺点：朴素贝叶斯对**强关联的词汇处理效果非常不好**。比如“关羽”和“青龙偃月刀”是两个关联度很高的词，如果没有考虑关联性，从而到贡献度重复计算的现象。

- 线性回归模型
$$ y=w_1 x_1 + w_2 x_2 ...w_n x_n + b $$
也许你会觉得线性回归模型很简单，但是实际上，深度学习本质就是对简单模型的升级和拓展。
当我们要找一条直线的时候，即是对我们的搜索进行限定。

- 梯度下降法

往梯度相反的方向去走，从而找到cost函数的最小值。
什么是梯度？
![等高线](http://static.zybuluo.com/lukesummer/phherl43zlsmjcs636yw8ih7/image.png)

梯度相关的文章：

[An overview of gradient descent optimization algorithms](http://sebastianruder.com/optimizing-gradient-descent/)
