---
title: 梯度下降
date: 2017-10-25 23:42:21
categories:
- 技术
tags:
- 深度学习
---
[TOC]

# Ch2: Naive Bayes和Gradient Descent
> 任务一：运用贝叶斯公式
任务二：实现 Navive Bayes 方法
任务三：实现 Gradient Descent 算法

## 情绪的理解
情绪的分析最基本的判断就是正面和负面的划分。
[snwnlp语料库](https://github.com/isnowfy/snownlp)
snowNLP是一个Python写的类库，可以方便地处理中文文本内容，一个方便用于处理中文的类库，并且没有NLTK（自然语言处理包），所有的算法都是自己实现的，并且自带了一些训练好了的字典。本程序都是处理unicode编码的，需要自行decode和unicode。

>推荐英文论文:Speech and Language Processing - ch6 - naive bayes and sentiment classification

情感分类可以分为正面和负面，更为细致的我们可以将其分为兴奋、悲伤等等。更一般的我们利用机器学习里面的有监督学习，通过人为标签告诉机器什么句子是正面的，什么句子是负面的。

## 有监督学习
有监督问题可以分为
- 回归问题：标签（因变量）是实数
- 分类问题：标签（因变量）是有限集合

当你不限定搜索集合的时候，最好的拟合模型就是数据本身。有监督学习是**限定集合**里的最优模型。

>[Andrew Ng描述有监督学习](https://www.coursera.org/learn/machine-learning/lecture/1VkCb/supervised-learning)

接下来我们来看看各个算法是如何设定所谓的限定集合的（有限参数可调整的函数）。

- 贝叶斯分类器

$$ P(y=emotion|S)=P(y|w_1, w_2, w_3, ..., w_n) $$
$$ f(S)=arg\ \max_{y} P(y|S)  $$
其中S表示句子。
因为条件不可拆解，所以我们可以利用贝叶斯公式，将情绪做条件，变成
$$ P(y|S)=\frac{P(S|y)P(y)}{P(S)} $$
以情绪为条件的不同句子的概率分布。
其中的分母P(S)并不需要去计算，不同的emotion，P(S)是一样的。
叠加词汇之间的独立性假设，我们就可以得到我们的朴素贝叶斯模型：
$$ P(S|y=emotion)=P(w_1, w_2, w_3, ..., w_n|y=emotion)
\approx P(w_1|y=emtion)...P(w_n|y=emotion) $$

注意：通过独立性假设之后，我们可以直接得到句子的情感。不需要任何的拟合优化，非常简单。

> **疑问**：
*为什么贝叶斯分类器分母P(S)在不同的emotion下都是一样的？*
答：因为这个公式的变量只有y，你要理解这个公式要解决什么给你一个句子，你要判断它属于各个情绪的概率，那么很显然，你要求的是y的概率分布，S是作为一种已知的常量代入的。即使我们不知道P(S)，最后也只需要利用所有y的概率表达式的和等于1的限制条件，就可以反算出P(S)的值。

>**作业1**：
- 利用贝叶斯公式，说明为什么$P(y|w_1w_2)!=P(y|w_1)P(y|w_2)$(即使做了独立假设）
- 自己用Python实现Naive Bayes方法，并在给定的数据集上验证效果（通过人眼去看看模型的效果是否是靠谱的）


梳理一下，朴素贝叶斯和一般贝叶斯分类器有什么区别？它为什么too naive?
答：朴素贝叶斯假设一个句子中词汇之间是没有相关性的，方便对贝叶斯公式进行分解，
>[数学之美番外篇：平凡而又神奇的贝叶斯方法 – 刘未鹏 | Mind Hacks](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/)
[朴素贝叶斯分类器的应用 - 阮一峰](http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html)

特征工程：通过人的知识，对数据进行拆解分析，相当于人为模型服务。在真正的数据挖掘项目中，数据标准化，特征工程以及模型设计是花时间最多的工作，一旦这些东西工作完成，剩下的用训练测试可能只占到整个宏观数据挖掘工程的5%左右。

缺点：朴素贝叶斯对**强关联的词汇处理效果非常不好**。比如“关羽”和“青龙偃月刀”是两个关联度很高的词，如果没有考虑关联性，从而到贡献度重复计算的现象。

- 线性回归模型
$$ y=w_1 x_1 + w_2 x_2 ...w_n x_n + b $$
也许你会觉得线性回归模型很简单，但是实际上，深度学习本质就是对简单模型的升级和拓展。
当我们要找一条直线的时候，即是对我们的搜索进行限定。

- 梯度下降法

往梯度相反的方向去走，从而找到cost函数的最小值。
什么是梯度？
![等高线](https://leanote.com/api/file/getImage?fileId=59f59b74ab644164490010f3)

梯度相关的文章：

[An overview of gradient descent optimization algorithms](http://sebastianruder.com/optimizing-gradient-descent/)

## 本周任务
### 1.任务一：运用贝叶斯公式

利用贝叶斯公式，说明为什么 P(y|w1, w2) ≠ P(y|w1)P(y|w2) （即使做了独立假设）

### 2. 任务二：实现Navive Bayes方法

请你用 Python 实现 Naive Bayes 方法，并在给定的数据集上验证数据。具体要求如下：

在「训练数据」上拟合一个 Naive Bayes 模型。在训练时模型不能「看见」任何测试数据的信息。
训练完成后，在测试数据上进行测试。评估标准为你的模型在测试数据上的混淆矩阵（Confusion Matrix）结果。
根据混淆矩阵的结果，分析一下你模型的表现。
参考概念：混淆矩阵 [Simple guide to confusion matrix terminology](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)

### 3. 任务三：实现Gradient Descent算法

通过梯度下降法，自己实现一种通用的给定数据找到 y = wx + b 中最优的 w 和 b 的程序，并用加噪音数据验证效果。

## 本周任务加油包
### 平滑与未登录词
如果我们在测试集中遇到了一个训练集当中没有出现过的词（未登录词，Out of vocabulary, OOV)，怎么办呢？

模型在训练阶段只能使用训练集的数据和信息，因此我们不能在测试时根据测试集的信息有任何的改动（这是非常重要的一点，有些学术论文就因为犯了这种错误，发布后撤稿）。因此不在训练集里的词，确实就是没有见过，那么对这些词模型是不是就只能说『无可奉告』了呢？

按照标准的概率模型，这个词可以说在训练数据中完全没见过，因此理论上似乎可以把这个词看成概率为0，这样造成的结果就是整个句子概率因为这一个0而在连乘之后变成0了。这样的话模型就显得过于『武断』了。

另外在实际实现当中，为了节省内存，我们可能不会存所有词的概率，而只会存储最高频的比如 8 万个词的概率。这种情况下，在测试阶段遇到没见过的词（其实可能只是没存）的情况又更多了。

对于这种问题，一种常见的解决方法，叫 Laplace (add-1) smoothing。假设训练集合得到的词个数是 V（可能就是训练集合中所有的词，也可能是做高频截断的词），我们另外为未登录词加一个『入口』，例如可以把所有不在训练词集合里的词标记为 OOV, 或者 Unknown。这时候真正的词表大小就由 V 变成了 V + 1。

加了这个词，概率还是 0。这时候的解决方案有点像大富翁游戏里的『均富卡』：『假装』让所有词都多出现一次！这时候每个词出现的『频率』都加 1，而因为有 V + 1 个词，分母会加 V + 1，这样才能保证所有词的概率加起来 = 1
$$ P(w_1|y=emotion) \approx \frac{Count(w_1, y=emotion)+1}{Count(y=emotion)+V+1} $$
